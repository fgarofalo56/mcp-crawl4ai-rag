# Docker-specific environment configuration
# This file is used when running the MCP server in Docker with HTTP/SSE transport

# Use SSE/HTTP transport for Docker (allows network access)
TRANSPORT=sse

# Bind to all interfaces for Docker (required for port mapping)
HOST=0.0.0.0

# Port to listen on (exposed via Docker)
PORT=8051

# Copy all other environment variables from your .env file below:
# (You should copy OPENAI_API_KEY, SUPABASE_URL, etc. from your .env)
# The transport for the MCP server - either 'sse' or 'stdio' (defaults to sse if left empty)
# Use 'stdio' for Claude Desktop, 'sse' for web/HTTP clients

# Host to bind to if using sse as the transport (leave empty if using stdio)
# Set this to 0.0.0.0 if using Docker, otherwise set to localhost (if using uv)


# Port to listen on if using sse as the transport (leave empty if using stdio)
# PORT=

# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# This is for the embedding model - text-embedding-3-small will be used
OPENAI_API_KEY=your-openai-api-key-here
# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here

AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_ENDPOINT=https://dml-ai-eastus-sandbox.cognitiveservices.azure.com
# The LLM you want to use for summaries and contextual embeddings
# Generally this is a very cheap and fast LLM like gpt-35-turbo,gpt-4.1, gpt-4o, gpt-4o-mini, gpt-5-mini
MODEL_CHOICE=gpt-4.1-nano
DEPLOYMENT=gpt-4.1-nano


# RAG strategies - set these to "true" or "false" (default to "false")
# USE_CONTEXTUAL_EMBEDDINGS: Enhances embeddings with contextual information for better retrieval
USE_CONTEXTUAL_EMBEDDINGS=true
EMBEDDING_DEPLOYMENT=text-embedding-3-small

# USE_HYBRID_SEARCH: Combines vector similarity search with keyword search for better results
USE_HYBRID_SEARCH=true

# USE_AGENTIC_RAG: Enables code example extraction, storage, and specialized code search functionality
USE_AGENTIC_RAG=true

# USE_RERANKING: Applies cross-encoder reranking to improve search result relevance
USE_RERANKING=true

# USE_KNOWLEDGE_GRAPH: Enables AI hallucination detection and repository parsing tools using Neo4j
# If you set this to true, you must also set the Neo4j environment variables below.
USE_KNOWLEDGE_GRAPH=true

# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=https://mvtydpzixmxphjsqjfoq.supabase.co
#  local_url: http://host.docker.internal:8000

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=your-supabase-service-key-here
# Local key:  eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoic2VydmljZV9yb2xlIiwiaXNzIjoic3VwYWJhc2UiLCJpYXQiOjE3NTI4MTEyMDAsImV4cCI6MTkxMDU3NzYwMH0.B0oMG8MucB1TVzW8fE58iJ2pVXSX9OLPI7YVoyyNBrc
# Neo4j Configuration for Knowledge Graph Tools
# These are required for the AI hallucination detection and repository parsing tools
# Leave empty to disable knowledge graph functionality

# Neo4j connection URI - use bolt://localhost:7687 for local, neo4j:// for cloud instances
# IMPORTANT: If running the MCP server through Docker, change localhost to host.docker.internal
NEO4J_URI=bolt://host.docker.internal:7687

# Neo4j username (usually 'neo4j' for default installations)
NEO4J_USER=neo4j

# Neo4j password for your database instance
NEO4J_PASSWORD=your-neo4j-password-here

