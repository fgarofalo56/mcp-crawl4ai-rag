"""
MCP server for web crawling with Crawl4AI.

This server provides tools to crawl websites using Crawl4AI, automatically detecting
the appropriate crawl method based on URL type (sitemap, txt file, or regular webpage).
Also includes AI hallucination detection and repository parsing tools using Neo4j knowledge graphs.
"""

# Suppress known deprecation warnings from dependencies BEFORE imports
import warnings

# Suppress Pydantic V2 deprecation warnings from third-party dependencies
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pydantic.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="crawl4ai.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="openapi_pydantic.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="supabase.*")

# Suppress specific warning messages
warnings.filterwarnings("ignore", message=".*class-based `config` is deprecated.*")
warnings.filterwarnings("ignore", message=".*Support for class-based.*")
warnings.filterwarnings("ignore", message=".*The `gotrue` package is deprecated.*")

# Suppress all Pydantic-related deprecation warnings at the category level
try:
    from pydantic import PydanticDeprecatedSince20
    warnings.filterwarnings("ignore", category=PydanticDeprecatedSince20)
except ImportError:
    pass

from fastmcp import FastMCP, Context
from sentence_transformers import CrossEncoder
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, urldefrag
from xml.etree import ElementTree
from dotenv import load_dotenv
from supabase import Client
from pathlib import Path
import requests
import asyncio
import json
import os
import re
import concurrent.futures
import sys

from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode,
    MemoryAdaptiveDispatcher,
)

# Add knowledge_graphs folder to path for importing knowledge graph modules
knowledge_graphs_path = Path(__file__).resolve().parent.parent / "knowledge_graphs"
sys.path.append(str(knowledge_graphs_path))

from .utils import (
    get_supabase_client,
    add_documents_to_supabase,
    search_documents,
    extract_code_blocks,
    generate_code_example_summary,
    add_code_examples_to_supabase,
    update_source_info,
    extract_source_summary,
    search_code_examples,
)

# Import knowledge graph modules (code repository graph)
from knowledge_graph_validator import KnowledgeGraphValidator
from parse_repo_into_neo4j import DirectNeo4jExtractor
from ai_script_analyzer import AIScriptAnalyzer
from hallucination_reporter import HallucinationReporter

# Import document graph modules (GraphRAG for web content)
from knowledge_graphs.document_graph_validator import DocumentGraphValidator
from knowledge_graphs.document_entity_extractor import DocumentEntityExtractor
from knowledge_graphs.document_graph_queries import DocumentGraphQueries

from dotenv import load_dotenv

load_dotenv()


# Load environment variables from the project root .env file
project_root = Path(__file__).resolve().parent.parent
dotenv_path = project_root / ".env"

# Load .env but don't override variables already set by run_mcp.py (like TRANSPORT and PORT)
load_dotenv(dotenv_path, override=False)


# Helper functions for Neo4j validation and error handling
def validate_neo4j_connection() -> bool:
    """Check if Neo4j environment variables are configured."""
    return all(
        [os.getenv("NEO4J_URI"), os.getenv("NEO4J_USER"), os.getenv("NEO4J_PASSWORD")]
    )


def format_neo4j_error(error: Exception) -> str:
    """Format Neo4j connection errors for user-friendly messages."""
    error_str = str(error).lower()
    if "authentication" in error_str or "unauthorized" in error_str:
        return "Neo4j authentication failed. Check NEO4J_USER and NEO4J_PASSWORD."
    elif "connection" in error_str or "refused" in error_str or "timeout" in error_str:
        return "Cannot connect to Neo4j. Check NEO4J_URI and ensure Neo4j is running."
    elif "database" in error_str:
        return "Neo4j database error. Check if the database exists and is accessible."
    else:
        return f"Neo4j error: {str(error)}"


def validate_script_path(script_path: str) -> Dict[str, Any]:
    """Validate script path and return error info if invalid."""
    if not script_path or not isinstance(script_path, str):
        return {"valid": False, "error": "Script path is required"}

    if not os.path.exists(script_path):
        return {"valid": False, "error": f"Script not found: {script_path}"}

    if not script_path.endswith(".py"):
        return {"valid": False, "error": "Only Python (.py) files are supported"}

    try:
        # Check if file is readable
        with open(script_path, "r", encoding="utf-8") as f:
            f.read(1)  # Read first character to test
        return {"valid": True}
    except Exception as e:
        return {"valid": False, "error": f"Cannot read script file: {str(e)}"}


def validate_github_url(repo_url: str) -> Dict[str, Any]:
    """Validate GitHub repository URL."""
    if not repo_url or not isinstance(repo_url, str):
        return {"valid": False, "error": "Repository URL is required"}

    repo_url = repo_url.strip()

    # Basic GitHub URL validation
    if not ("github.com" in repo_url.lower() or repo_url.endswith(".git")):
        return {"valid": False, "error": "Please provide a valid GitHub repository URL"}

    # Check URL format
    if not (repo_url.startswith("https://") or repo_url.startswith("git@")):
        return {
            "valid": False,
            "error": "Repository URL must start with https:// or git@",
        }

    return {"valid": True, "repo_name": repo_url.split("/")[-1].replace(".git", "")}


# Create a dataclass for our application context
@dataclass
class Crawl4AIContext:
    """Context for the Crawl4AI MCP server."""

    crawler: AsyncWebCrawler
    supabase_client: Client
    reranking_model: Optional[Any] = None  # CrossEncoder or LazyReranker when available
    knowledge_validator: Optional[Any] = None  # KnowledgeGraphValidator when available
    repo_extractor: Optional[Any] = None  # DirectNeo4jExtractor when available
    # GraphRAG components (document knowledge graph)
    document_graph_validator: Optional[Any] = None  # DocumentGraphValidator when available
    document_entity_extractor: Optional[Any] = None  # DocumentEntityExtractor when available
    document_graph_queries: Optional[Any] = None  # DocumentGraphQueries when available


@asynccontextmanager
async def crawl4ai_lifespan(server: FastMCP) -> AsyncIterator[Crawl4AIContext]:
    """
    Manages the Crawl4AI client lifecycle with proper resource cleanup.

    Args:
        server: The FastMCP server instance

    Yields:
        Crawl4AIContext: The context containing the Crawl4AI crawler and Supabase client
    """
    # Import initialization utilities
    from initialization_utils import (
        initialize_supabase,
        initialize_reranker,
        initialize_knowledge_graph,
        initialize_graphrag,
        cleanup_knowledge_graph,
        cleanup_graphrag,
    )

    # Initialize all resources to None for proper cleanup tracking
    crawler = None
    supabase_client = None
    reranking_model = None
    knowledge_validator = None
    repo_extractor = None
    document_graph_validator = None
    document_entity_extractor = None
    document_graph_queries = None

    print("üîß Starting MCP server initialization...", file=sys.stderr, flush=True)

    try:
        # Create browser configuration
        print("üîß Creating browser configuration...", file=sys.stderr, flush=True)
        browser_config = BrowserConfig(headless=True, verbose=False)

        # Initialize the crawler
        print("üîß Initializing Crawl4AI browser...", file=sys.stderr, flush=True)
        crawler = AsyncWebCrawler(config=browser_config)
        await crawler.__aenter__()
        print("‚úì Crawl4AI browser ready", file=sys.stderr, flush=True)

        # Initialize Supabase client
        print("üîß Connecting to Supabase...", file=sys.stderr, flush=True)
        supabase_client = initialize_supabase()
        print("‚úì Supabase connected", file=sys.stderr, flush=True)

        # Initialize reranking model
        reranking_model = initialize_reranker()

        # Initialize Neo4j knowledge graph components
        knowledge_validator, repo_extractor = await initialize_knowledge_graph()

        # Initialize GraphRAG components
        document_graph_validator, document_entity_extractor, document_graph_queries = (
            await initialize_graphrag()
        )

        print("‚úì MCP server initialization complete!", file=sys.stderr, flush=True)

        yield Crawl4AIContext(
            crawler=crawler,
            supabase_client=supabase_client,
            reranking_model=reranking_model,
            knowledge_validator=knowledge_validator,
            repo_extractor=repo_extractor,
            document_graph_validator=document_graph_validator,
            document_entity_extractor=document_entity_extractor,
            document_graph_queries=document_graph_queries,
        )
    finally:
        # Clean up crawler with error handling
        if crawler:
            try:
                print("üîß Closing Crawl4AI browser...", file=sys.stderr, flush=True)
                await crawler.__aexit__(None, None, None)
                print("‚úì Crawl4AI browser closed", file=sys.stderr, flush=True)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error closing crawler: {e}", file=sys.stderr, flush=True)

        # Clean up knowledge graph components with error handling
        if knowledge_validator or repo_extractor:
            try:
                print("üîß Cleaning up knowledge graph...", file=sys.stderr, flush=True)
                await cleanup_knowledge_graph(knowledge_validator, repo_extractor)
                print("‚úì Knowledge graph cleaned up", file=sys.stderr, flush=True)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error cleaning up knowledge graph: {e}", file=sys.stderr, flush=True)

        # Clean up GraphRAG components with error handling
        if document_graph_validator or document_graph_queries:
            try:
                print("üîß Cleaning up GraphRAG...", file=sys.stderr, flush=True)
                await cleanup_graphrag(document_graph_validator, document_graph_queries)
                print("‚úì GraphRAG cleaned up", file=sys.stderr, flush=True)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error cleaning up GraphRAG: {e}", file=sys.stderr, flush=True)

        print("‚úì MCP server shutdown complete", file=sys.stderr, flush=True)


# Initialize FastMCP server
mcp = FastMCP(
    name="mcp-crawl4ai-rag",
    lifespan=crawl4ai_lifespan,
)


def rerank_results(
    model: CrossEncoder,
    query: str,
    results: List[Dict[str, Any]],
    content_key: str = "content",
) -> List[Dict[str, Any]]:
    """
    Rerank search results using a cross-encoder model.

    Args:
        model: The cross-encoder model to use for reranking
        query: The search query
        results: List of search results
        content_key: The key in each result dict that contains the text content

    Returns:
        Reranked list of results
    """
    if not model or not results:
        return results

    try:
        # Extract content from results
        texts = [result.get(content_key, "") for result in results]

        # Create pairs of [query, document] for the cross-encoder
        pairs = [[query, text] for text in texts]

        # Get relevance scores from the cross-encoder
        scores = model.predict(pairs)

        # Add scores to results and sort by score (descending)
        for i, result in enumerate(results):
            result["rerank_score"] = float(scores[i])

        # Sort by rerank score
        reranked = sorted(results, key=lambda x: x.get("rerank_score", 0), reverse=True)

        return reranked
    except Exception as e:
        print(f"Error during reranking: {e}", file=sys.stderr, flush=True)
        return results


def is_sitemap(url: str) -> bool:
    """
    Check if a URL is a sitemap.

    Args:
        url: URL to check

    Returns:
        True if the URL is a sitemap, False otherwise
    """
    return url.endswith("sitemap.xml") or "sitemap" in urlparse(url).path


def is_txt(url: str) -> bool:
    """
    Check if a URL is a text file.

    Args:
        url: URL to check

    Returns:
        True if the URL is a text file, False otherwise
    """
    return url.endswith(".txt")


def parse_sitemap(sitemap_url: str) -> List[str]:
    """
    Parse a sitemap and extract URLs.

    Args:
        sitemap_url: URL of the sitemap

    Returns:
        List of URLs found in the sitemap
    """
    resp = requests.get(sitemap_url)
    urls = []

    if resp.status_code == 200:
        try:
            tree = ElementTree.fromstring(resp.content)
            urls = [loc.text for loc in tree.findall(".//{*}loc")]
        except Exception as e:
            print(f"Error parsing sitemap XML: {e}", file=sys.stderr, flush=True)

    return urls


def smart_chunk_markdown(text: str, chunk_size: int = 5000) -> List[str]:
    """Split text into chunks, respecting code blocks and paragraphs."""
    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        # Calculate end position
        end = start + chunk_size

        # If we're at the end of the text, just take what's left
        if end >= text_length:
            chunks.append(text[start:].strip())
            break

        # Try to find a code block boundary first (```)
        chunk = text[start:end]
        code_block = chunk.rfind("```")
        if code_block != -1 and code_block > chunk_size * 0.3:
            end = start + code_block

        # If no code block, try to break at a paragraph
        elif "\n\n" in chunk:
            # Find the last paragraph break
            last_break = chunk.rfind("\n\n")
            if (
                last_break > chunk_size * 0.3
            ):  # Only break if we're past 30% of chunk_size
                end = start + last_break

        # If no paragraph break, try to break at a sentence
        elif ". " in chunk:
            # Find the last sentence break
            last_period = chunk.rfind(". ")
            if (
                last_period > chunk_size * 0.3
            ):  # Only break if we're past 30% of chunk_size
                end = start + last_period + 1

        # Extract chunk and clean it up
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        # Move start position for next chunk
        start = end

    return chunks


def extract_section_info(chunk: str) -> Dict[str, Any]:
    """
    Extracts headers and stats from a chunk.

    Args:
        chunk: Markdown chunk

    Returns:
        Dictionary with headers and stats
    """
    headers = re.findall(r"^(#+)\s+(.+)$", chunk, re.MULTILINE)
    header_str = "; ".join([f"{h[0]} {h[1]}" for h in headers]) if headers else ""

    return {
        "headers": header_str,
        "char_count": len(chunk),
        "word_count": len(chunk.split()),
    }


def process_code_example(args):
    """
    Process a single code example to generate its summary.
    This function is designed to be used with concurrent.futures.

    Args:
        args: Tuple containing (code, context_before, context_after)

    Returns:
        The generated summary
    """
    code, context_before, context_after = args
    return generate_code_example_summary(code, context_before, context_after)


@mcp.tool()
async def crawl_single_page(ctx: Context, url: str) -> str:
    """
    Crawl a single web page and store its content in Supabase.

    This tool is ideal for quickly retrieving content from a specific URL without following links.
    The content is stored in Supabase for later retrieval and querying.

    Args:
        ctx: The MCP server provided context
        url: URL of the web page to crawl

    Returns:
        Summary of the crawling operation and storage in Supabase
    """
    # Import crawl helpers
    from crawl_helpers import (
        validate_crawl_url,
        crawl_and_extract_content,
        chunk_and_prepare_documents,
        extract_and_process_code_examples,
        store_crawl_results,
        store_code_examples,
        should_extract_code_examples,
    )

    try:
        # Validate URL
        validation = validate_crawl_url(url)
        if not validation["valid"]:
            return json.dumps(
                {"success": False, "url": url, "error": validation["error"]}, indent=2
            )

        # Get the crawler and client from the context
        crawler = ctx.request_context.lifespan_context.crawler
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Crawl and extract content
        success, markdown_content, metadata = await crawl_and_extract_content(
            crawler, url
        )

        if not success:
            return json.dumps(
                {"success": False, "url": url, "error": metadata.get("error", "Unknown error")},
                indent=2,
            )

        # Get source_id from validation
        source_id = validation["source_id"]

        # Chunk and prepare documents
        urls, chunk_numbers, contents, metadatas, total_word_count = (
            chunk_and_prepare_documents(url, markdown_content, source_id)
        )

        # Create url_to_full_document mapping
        url_to_full_document = {url: markdown_content}

        # Extract source summary and store results
        source_summary = extract_source_summary(source_id, markdown_content[:5000])
        store_crawl_results(
            supabase_client,
            urls,
            chunk_numbers,
            contents,
            metadatas,
            url_to_full_document,
            source_id,
            total_word_count,
            source_summary,
        )

        # Process code examples if enabled
        code_examples_count = 0
        if should_extract_code_examples():
            (
                code_urls,
                code_chunk_numbers,
                code_examples,
                code_summaries,
                code_metadatas,
            ) = extract_and_process_code_examples(url, markdown_content, source_id)

            if code_examples:
                store_code_examples(
                    supabase_client,
                    code_urls,
                    code_chunk_numbers,
                    code_examples,
                    code_summaries,
                    code_metadatas,
                )
                code_examples_count = len(code_examples)

        return json.dumps(
            {
                "success": True,
                "url": url,
                "chunks_stored": len(contents),
                "code_examples_stored": code_examples_count,
                "content_length": metadata["content_length"],
                "total_word_count": total_word_count,
                "source_id": source_id,
                "links_count": metadata["links"],
            },
            indent=2,
        )

    except Exception as e:
        return json.dumps({"success": False, "url": url, "error": str(e)}, indent=2)


@mcp.tool()
async def crawl_with_stealth_mode(
    ctx: Context,
    url: str,
    max_depth: int = 3,
    max_concurrent: int = 10,
    chunk_size: int = 5000,
    wait_for_selector: str = "",
    extra_wait: int = 2
) -> str:
    """
    Crawl URLs using undetected browser mode to bypass bot protection (Cloudflare, Akamai, etc.).

    This tool uses stealth browser technology to appear as a regular user, making it ideal for:
    - Sites with Cloudflare protection
    - Sites with bot detection (Akamai, PerimeterX, etc.)
    - Sites that block headless browsers
    - Content behind aggressive anti-scraping measures

    Args:
        ctx: The MCP server provided context
        url: URL to crawl (can be a regular webpage, sitemap.xml, or .txt file)
        max_depth: Maximum recursion depth for regular URLs (default: 3)
        max_concurrent: Maximum number of concurrent browser sessions (default: 10)
        chunk_size: Maximum size of each content chunk in characters (default: 5000)
        wait_for_selector: Optional CSS selector to wait for before extracting content
        extra_wait: Additional wait time in seconds after page load (default: 2)

    Returns:
        JSON string with crawl summary, storage information, and success statistics

    Example:
        # Bypass Cloudflare-protected site
        crawl_with_stealth_mode("https://example.com", wait_for_selector="div.content", extra_wait=3)
    """
    try:
        # Get supabase client from context
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Configure undetected browser for stealth mode
        browser_config = BrowserConfig(
            browser_type="undetected",
            headless=True,
            verbose=False,
            extra_args=["--disable-blink-features=AutomationControlled"],
        )

        # Initialize crawler with stealth configuration and execute strategy
        from crawling_strategies import CrawlingStrategyFactory

        async with AsyncWebCrawler(config=browser_config) as stealth_crawler:
            strategy = CrawlingStrategyFactory.get_strategy(url)
            crawl_result = await strategy.crawl(
                crawler=stealth_crawler,
                url=url,
                max_depth=max_depth,
                max_concurrent=max_concurrent,
            )

            # Handle crawl failures
            if not crawl_result.success:
                return json.dumps(
                    {
                        "success": False,
                        "url": url,
                        "error": crawl_result.error_message or "No content found",
                    },
                    indent=2,
                )

            # Process and store results
            storage_stats = process_and_store_crawl_results(
                supabase_client=supabase_client,
                crawl_results=crawl_result.documents,
                crawl_type=f"stealth_{crawl_result.metadata.get('strategy', 'unknown')}",
                chunk_size=chunk_size,
            )

            return json.dumps(
                {
                    "success": True,
                    "url": url,
                    "mode": "stealth (undetected browser)",
                    "crawl_type": f"stealth_{crawl_result.metadata.get('strategy', 'unknown')}",
                    "pages_crawled": crawl_result.pages_crawled,
                    "chunks_stored": storage_stats["chunks_stored"],
                    "code_examples_stored": storage_stats["code_examples_stored"],
                    "sources_updated": storage_stats["sources_updated"],
                },
                indent=2,
            )

    except Exception as e:
        return json.dumps({"success": False, "url": url, "error": str(e)}, indent=2)


@mcp.tool()
async def smart_crawl_url(
    ctx: Context,
    url: str,
    max_depth: int = 3,
    max_concurrent: int = 10,
    chunk_size: int = 5000,
) -> str:
    """
    Intelligently crawl a URL based on its type and store content in Supabase.

    This tool automatically detects the URL type and applies the appropriate crawling method:
    - For sitemaps: Extracts and crawls all URLs in parallel
    - For text files (llms.txt): Directly retrieves the content
    - For regular webpages: Recursively crawls internal links up to the specified depth

    All crawled content is chunked and stored in Supabase for later retrieval and querying.

    Args:
        ctx: The MCP server provided context
        url: URL to crawl (can be a regular webpage, sitemap.xml, or .txt file)
        max_depth: Maximum recursion depth for regular URLs (default: 3)
        max_concurrent: Maximum number of concurrent browser sessions (default: 10)
        chunk_size: Maximum size of each content chunk in characters (default: 5000)

    Returns:
        JSON string with crawl summary and storage information
    """
    try:
        # Get clients from context
        crawler = ctx.request_context.lifespan_context.crawler
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Get appropriate strategy and execute crawl
        from crawling_strategies import CrawlingStrategyFactory

        strategy = CrawlingStrategyFactory.get_strategy(url)
        crawl_result = await strategy.crawl(
            crawler=crawler,
            url=url,
            max_depth=max_depth,
            max_concurrent=max_concurrent,
        )

        # Handle crawl failures
        if not crawl_result.success:
            return json.dumps(
                {
                    "success": False,
                    "url": url,
                    "error": crawl_result.error_message or "No content found",
                },
                indent=2,
            )

        # Process and store results using helper function
        storage_stats = process_and_store_crawl_results(
            supabase_client=supabase_client,
            crawl_results=crawl_result.documents,
            crawl_type=crawl_result.metadata.get("strategy", "unknown"),
            chunk_size=chunk_size,
        )

        # Return success response
        return json.dumps(
            {
                "success": True,
                "url": url,
                "crawl_type": crawl_result.metadata.get("strategy", "unknown"),
                "pages_crawled": crawl_result.pages_crawled,
                "chunks_stored": storage_stats["chunks_stored"],
                "code_examples_stored": storage_stats["code_examples_stored"],
                "sources_updated": storage_stats["sources_updated"],
                "urls_crawled": [doc["url"] for doc in crawl_result.documents][:5]
                + (["..."] if len(crawl_result.documents) > 5 else []),
            },
            indent=2,
        )
    except Exception as e:
        return json.dumps({"success": False, "url": url, "error": str(e)}, indent=2)


@mcp.tool()
async def crawl_with_multi_url_config(
    ctx: Context,
    urls_json: str,
    max_concurrent: int = 5,
    chunk_size: int = 5000
) -> str:
    """
    Crawl multiple URLs with smart per-URL configuration based on content type patterns.

    This tool automatically optimizes crawler settings for different types of content:
    - Documentation sites: Wait for code blocks, extra parsing time
    - News/articles: Focus on main content, minimal wait
    - E-commerce: Wait for dynamic pricing, product details
    - Forums/discussions: Handle infinite scroll, wait for comments

    Args:
        ctx: The MCP server provided context
        urls_json: JSON array of URLs to crawl with smart configuration
                   Example: '["https://docs.python.org", "https://news.example.com"]'
        max_concurrent: Maximum number of concurrent browser sessions (default: 5)
        chunk_size: Maximum size of each content chunk in characters (default: 5000)

    Returns:
        JSON string with crawl summary for each URL and aggregate statistics

    Example:
        # Crawl multiple site types with optimized settings
        urls = '["https://docs.example.com", "https://api.example.com"]'
        crawl_with_multi_url_config(urls)
    """
    try:
        # Get clients from context
        crawler = ctx.request_context.lifespan_context.crawler
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Parse URL list
        try:
            url_list = json.loads(urls_json)
            if not isinstance(url_list, list):
                return json.dumps({"error": "urls_json must be a JSON array of URLs"})
        except json.JSONDecodeError as e:
            return json.dumps({"error": f"Invalid JSON: {str(e)}"})

        # Import strategy factory
        from crawling_strategies import CrawlingStrategyFactory

        results = []
        total_chunks = 0
        total_code_examples = 0
        total_sources = 0

        # Process each URL using strategy pattern
        for url in url_list:
            # Determine content type for metadata
            if any(kw in url.lower() for kw in ["docs", "documentation", "api", "reference"]):
                content_type = "documentation"
            elif any(kw in url.lower() for kw in ["news", "article", "blog", "post"]):
                content_type = "article"
            else:
                content_type = "general"

            # Execute crawl using appropriate strategy
            strategy = CrawlingStrategyFactory.get_strategy(url)
            crawl_result = await strategy.crawl(
                crawler=crawler,
                url=url,
                max_depth=2,
                max_concurrent=max_concurrent,
            )

            # Handle failures
            if not crawl_result.success:
                results.append({
                    "url": url,
                    "content_type": content_type,
                    "success": False,
                    "error": crawl_result.error_message or "No content found"
                })
                continue

            # Process and store results
            storage_stats = process_and_store_crawl_results(
                supabase_client=supabase_client,
                crawl_results=crawl_result.documents,
                crawl_type="multi_url",
                chunk_size=chunk_size,
            )

            # Aggregate stats
            results.append({
                "url": url,
                "content_type": content_type,
                "success": True,
                "pages_crawled": crawl_result.pages_crawled,
                "chunks_stored": storage_stats["chunks_stored"],
                "code_examples_stored": storage_stats["code_examples_stored"],
            })
            total_chunks += storage_stats["chunks_stored"]
            total_code_examples += storage_stats["code_examples_stored"]
            total_sources += storage_stats["sources_updated"]

        return json.dumps(
            {
                "success": True,
                "urls_processed": len(url_list),
                "total_chunks": total_chunks,
                "total_code_examples": total_code_examples,
                "total_sources": total_sources,
                "results": results
            },
            indent=2,
        )

    except Exception as e:
        return json.dumps({"success": False, "error": str(e)}, indent=2)


@mcp.tool()
async def crawl_with_memory_monitoring(
    ctx: Context,
    url: str,
    max_depth: int = 3,
    max_concurrent: int = 10,
    chunk_size: int = 5000,
    memory_threshold_mb: int = 500
) -> str:
    """
    Crawl URLs with active memory monitoring and adaptive throttling.

    This tool monitors memory usage during large-scale crawling operations and automatically
    adjusts concurrency to prevent memory exhaustion. Ideal for:
    - Large-scale documentation sites (1000+ pages)
    - Sites with heavy media content
    - Long-running crawl operations
    - Resource-constrained environments

    Args:
        ctx: The MCP server provided context
        url: URL to crawl (sitemap, webpage, or text file)
        max_depth: Maximum recursion depth (default: 3)
        max_concurrent: Initial concurrent sessions (auto-adjusted, default: 10)
        chunk_size: Chunk size in characters (default: 5000)
        memory_threshold_mb: Memory limit in MB before throttling (default: 500)

    Returns:
        JSON string with crawl summary and memory statistics

    Example:
        # Crawl large site with memory monitoring
        crawl_with_memory_monitoring("https://docs.example.com/sitemap.xml", memory_threshold_mb=300)
    """
    try:
        # Get clients from context
        crawler = ctx.request_context.lifespan_context.crawler
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Use MemoryMonitor context manager
        from memory_monitor import MemoryMonitor

        async with MemoryMonitor(threshold_mb=memory_threshold_mb) as monitor:
            # Execute crawl using strategy pattern
            from crawling_strategies import CrawlingStrategyFactory

            strategy = CrawlingStrategyFactory.get_strategy(url)
            crawl_result = await strategy.crawl(
                crawler=crawler,
                url=url,
                max_depth=max_depth,
                max_concurrent=max_concurrent,
            )

            # Handle crawl failures
            if not crawl_result.success:
                return json.dumps(
                    {
                        "success": False,
                        "url": url,
                        "error": crawl_result.error_message or "No content found",
                    },
                    indent=2,
                )

            # Process and store results
            storage_stats = process_and_store_crawl_results(
                supabase_client=supabase_client,
                crawl_results=crawl_result.documents,
                crawl_type=f"memory_monitored_{crawl_result.metadata.get('strategy', 'unknown')}",
                chunk_size=chunk_size,
            )

            # Get final memory statistics
            memory_stats = monitor.stats.to_dict()

            return json.dumps(
                {
                    "success": True,
                    "url": url,
                    "crawl_type": f"memory_monitored_{crawl_result.metadata.get('strategy', 'unknown')}",
                    "pages_crawled": crawl_result.pages_crawled,
                    "chunks_stored": storage_stats["chunks_stored"],
                    "code_examples_stored": storage_stats["code_examples_stored"],
                    "sources_updated": storage_stats["sources_updated"],
                    "memory_stats": memory_stats,
                },
                indent=2,
            )

    except ImportError as e:
        return json.dumps({
            "success": False,
            "error": f"Memory monitoring requires psutil library. Install with: pip install psutil. Error: {str(e)}"
        }, indent=2)
    except Exception as e:
        return json.dumps({"success": False, "url": url, "error": str(e)}, indent=2)


@mcp.tool()
async def get_available_sources(ctx: Context) -> str:
    """
    Get all available sources from the sources table.

    This tool returns a list of all unique sources (domains) that have been crawled and stored
    in the database, along with their summaries and statistics. This is useful for discovering
    what content is available for querying.

    Always use this tool before calling the RAG query or code example query tool
    with a specific source filter!

    Args:
        ctx: The MCP server provided context

    Returns:
        JSON string with the list of available sources and their details
    """
    try:
        # Get the Supabase client from the context
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Query the sources table directly
        result = (
            supabase_client.from_("sources").select("*").order("source_id").execute()
        )

        # Format the sources with their details
        sources = []
        if result.data:
            for source in result.data:
                sources.append(
                    {
                        "source_id": source.get("source_id"),
                        "summary": source.get("summary"),
                        "total_words": source.get("total_words"),
                        "created_at": source.get("created_at"),
                        "updated_at": source.get("updated_at"),
                    }
                )

        return json.dumps(
            {"success": True, "sources": sources, "count": len(sources)}, indent=2
        )
    except Exception as e:
        return json.dumps({"success": False, "error": str(e)}, indent=2)


@mcp.tool()
async def perform_rag_query(
    ctx: Context, query: str, source_filter: str = None, match_count: int = 5
) -> str:
    """
    Perform a RAG (Retrieval Augmented Generation) query on the stored content.

    This tool searches the vector database for content relevant to the query and returns
    the matching documents. Optionally filter by source domain.
    Get the source by using the get_available_sources tool before calling this search!

    Args:
        ctx: The MCP server provided context
        query: The search query
        source_filter: Optional source domain to filter results (e.g., 'example.com')
        match_count: Maximum number of results to return (default: 5)

    Returns:
        JSON string with the search results
    """
    # Import RAG utilities
    from .rag_utils import (
        perform_hybrid_search_for_documents,
        format_rag_results,
        build_rag_response,
        build_rag_error_response,
    )

    try:
        # Get the Supabase client from the context
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Check if hybrid search is enabled
        use_hybrid_search = os.getenv("USE_HYBRID_SEARCH", "false") == "true"

        # Prepare filter if source is provided and not empty
        filter_metadata = None
        if source_filter and source_filter.strip():
            filter_metadata = {"source": source_filter}

        # Execute search based on mode
        if use_hybrid_search:
            results = perform_hybrid_search_for_documents(
                supabase_client,
                query,
                source_filter,
                match_count,
                filter_metadata,
                search_documents
            )
        else:
            # Standard vector search only
            results = search_documents(
                client=supabase_client,
                query=query,
                match_count=match_count,
                filter_metadata=filter_metadata,
            )

        # Apply reranking if enabled
        use_reranking = os.getenv("USE_RERANKING", "false") == "true"
        if use_reranking and ctx.request_context.lifespan_context.reranking_model:
            results = rerank_results(
                ctx.request_context.lifespan_context.reranking_model,
                query,
                results,
                content_key="content",
            )

        # Format results and build response
        formatted_results = format_rag_results(results)
        return build_rag_response(
            query,
            source_filter,
            formatted_results,
            use_hybrid_search,
            use_reranking,
            ctx.request_context.lifespan_context.reranking_model is not None
        )

    except Exception as e:
        return build_rag_error_response(query, e)


@mcp.tool()
async def search_code_examples(
    ctx: Context, query: str, source_id: str = None, match_count: int = 5
) -> str:
    """
    Search for code examples relevant to the query.

    This tool searches the vector database for code examples relevant to the query and returns
    the matching examples with their summaries. Optionally filter by source_id.
    Get the source_id by using the get_available_sources tool before calling this search!

    Use the get_available_sources tool first to see what sources are available for filtering.

    Args:
        ctx: The MCP server provided context
        query: The search query
        source_id: Optional source ID to filter results (e.g., 'example.com')
        match_count: Maximum number of results to return (default: 5)

    Returns:
        JSON string with the search results
    """
    # Import search utilities
    from .search_utils import (
        check_code_examples_enabled,
        prepare_source_filter,
        perform_hybrid_search,
        execute_vector_search,
        format_search_results,
        build_search_response,
        build_error_response,
    )

    # Check if code example extraction is enabled
    enabled, error_msg = check_code_examples_enabled()
    if not enabled:
        return error_msg

    try:
        # Get the Supabase client from the context
        supabase_client = ctx.request_context.lifespan_context.supabase_client

        # Check if hybrid search is enabled
        use_hybrid_search = os.getenv("USE_HYBRID_SEARCH", "false") == "true"

        # Prepare filter metadata
        filter_metadata = prepare_source_filter(source_id)

        # Import the search function from utils
        from .utils import search_code_examples as search_code_examples_impl

        # Execute search based on mode
        if use_hybrid_search:
            results = perform_hybrid_search(
                supabase_client,
                query,
                source_id,
                match_count,
                filter_metadata,
                search_code_examples_impl
            )
        else:
            results = execute_vector_search(
                supabase_client,
                query,
                match_count,
                filter_metadata,
                search_code_examples_impl
            )

        # Apply reranking if enabled
        use_reranking = os.getenv("USE_RERANKING", "false") == "true"
        if use_reranking and ctx.request_context.lifespan_context.reranking_model:
            results = rerank_results(
                ctx.request_context.lifespan_context.reranking_model,
                query,
                results,
                content_key="content",
            )

        # Format results and build response
        formatted_results = format_search_results(results)
        return build_search_response(
            query,
            source_id,
            formatted_results,
            use_hybrid_search,
            use_reranking,
            ctx.request_context.lifespan_context.reranking_model is not None
        )

    except Exception as e:
        return build_error_response(query, e)


@mcp.tool()
async def check_ai_script_hallucinations(ctx: Context, script_path: str) -> str:
    """
    Check an AI-generated Python script for hallucinations using the knowledge graph.

    This tool analyzes a Python script for potential AI hallucinations by validating
    imports, method calls, class instantiations, and function calls against a Neo4j
    knowledge graph containing real repository data.

    The tool performs comprehensive analysis including:
    - Import validation against known repositories
    - Method call validation on classes from the knowledge graph
    - Class instantiation parameter validation
    - Function call parameter validation
    - Attribute access validation

    Args:
        ctx: The MCP server provided context
        script_path: Absolute path to the Python script to analyze

    Returns:
        JSON string with hallucination detection results, confidence scores, and recommendations
    """
    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps(
                {
                    "success": False,
                    "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment.",
                },
                indent=2,
            )

        # Get the knowledge validator from context (lazy-loaded)
        knowledge_validator_lazy = ctx.request_context.lifespan_context.knowledge_validator

        if not knowledge_validator_lazy:
            return json.dumps(
                {
                    "success": False,
                    "error": "Knowledge graph validator not available. Check Neo4j configuration in environment variables.",
                },
                indent=2,
            )

        # Lazy-load the actual validator
        knowledge_validator = await knowledge_validator_lazy.get_validator()
        if not knowledge_validator:
            return json.dumps(
                {
                    "success": False,
                    "error": "Failed to initialize knowledge graph validator.",
                },
                indent=2,
            )

        # Validate script path
        validation = validate_script_path(script_path)
        if not validation["valid"]:
            return json.dumps(
                {
                    "success": False,
                    "script_path": script_path,
                    "error": validation["error"],
                },
                indent=2,
            )

        # Step 1: Analyze script structure using AST
        analyzer = AIScriptAnalyzer()
        analysis_result = analyzer.analyze_script(script_path)

        if analysis_result.errors:
            print(f"Analysis warnings for {script_path}: {analysis_result.errors}", file=sys.stderr, flush=True)

        # Step 2: Validate against knowledge graph
        validation_result = await knowledge_validator.validate_script(analysis_result)

        # Step 3: Generate comprehensive report
        reporter = HallucinationReporter()
        report = reporter.generate_comprehensive_report(validation_result)

        # Format response with comprehensive information
        return json.dumps(
            {
                "success": True,
                "script_path": script_path,
                "overall_confidence": validation_result.overall_confidence,
                "validation_summary": {
                    "total_validations": report["validation_summary"][
                        "total_validations"
                    ],
                    "valid_count": report["validation_summary"]["valid_count"],
                    "invalid_count": report["validation_summary"]["invalid_count"],
                    "uncertain_count": report["validation_summary"]["uncertain_count"],
                    "not_found_count": report["validation_summary"]["not_found_count"],
                    "hallucination_rate": report["validation_summary"][
                        "hallucination_rate"
                    ],
                },
                "hallucinations_detected": report["hallucinations_detected"],
                "recommendations": report["recommendations"],
                "analysis_metadata": {
                    "total_imports": report["analysis_metadata"]["total_imports"],
                    "total_classes": report["analysis_metadata"]["total_classes"],
                    "total_methods": report["analysis_metadata"]["total_methods"],
                    "total_attributes": report["analysis_metadata"]["total_attributes"],
                    "total_functions": report["analysis_metadata"]["total_functions"],
                },
                "libraries_analyzed": report.get("libraries_analyzed", []),
            },
            indent=2,
        )

    except Exception as e:
        return json.dumps(
            {
                "success": False,
                "script_path": script_path,
                "error": f"Analysis failed: {str(e)}",
            },
            indent=2,
        )


@mcp.tool()
async def query_knowledge_graph(ctx: Context, command: str) -> str:
    """
    Query and explore the Neo4j knowledge graph containing repository data.

    This tool provides comprehensive access to the knowledge graph for exploring repositories,
    classes, methods, functions, and their relationships. Perfect for understanding what data
    is available for hallucination detection and debugging validation results.

    **‚ö†Ô∏è IMPORTANT: Always start with the `repos` command first!**
    Before using any other commands, run `repos` to see what repositories are available
    in your knowledge graph. This will help you understand what data you can explore.

    ## Available Commands:

    **Repository Commands:**
    - `repos` - **START HERE!** List all repositories in the knowledge graph
    - `explore <repo_name>` - Get detailed overview of a specific repository

    **Class Commands:**
    - `classes` - List all classes across all repositories (limited to 20)
    - `classes <repo_name>` - List classes in a specific repository
    - `class <class_name>` - Get detailed information about a specific class including methods and attributes

    **Method Commands:**
    - `method <method_name>` - Search for methods by name across all classes
    - `method <method_name> <class_name>` - Search for a method within a specific class

    **Custom Query:**
    - `query <cypher_query>` - Execute a custom Cypher query (results limited to 20 records)

    ## Knowledge Graph Schema:

    **Node Types:**
    - Repository: `(r:Repository {name: string})`
    - File: `(f:File {path: string, module_name: string})`
    - Class: `(c:Class {name: string, full_name: string})`
    - Method: `(m:Method {name: string, params_list: [string], params_detailed: [string], return_type: string, args: [string]})`
    - Function: `(func:Function {name: string, params_list: [string], params_detailed: [string], return_type: string, args: [string]})`
    - Attribute: `(a:Attribute {name: string, type: string})`

    **Relationships:**
    - `(r:Repository)-[:CONTAINS]->(f:File)`
    - `(f:File)-[:DEFINES]->(c:Class)`
    - `(c:Class)-[:HAS_METHOD]->(m:Method)`
    - `(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)`
    - `(f:File)-[:DEFINES]->(func:Function)`

    ## Example Workflow:
    ```
    1. repos                                    # See what repositories are available
    2. explore pydantic-ai                      # Explore a specific repository
    3. classes pydantic-ai                      # List classes in that repository
    4. class Agent                              # Explore the Agent class
    5. method run_stream                        # Search for run_stream method
    6. method __init__ Agent                    # Find Agent constructor
    7. query "MATCH (c:Class)-[:HAS_METHOD]->(m:Method) WHERE m.name = 'run' RETURN c.name, m.name LIMIT 5"
    ```

    Args:
        ctx: The MCP server provided context
        command: Command string to execute (see available commands above)

    Returns:
        JSON string with query results, statistics, and metadata
    """
    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps(
                {
                    "success": False,
                    "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment.",
                },
                indent=2,
            )

        # Get Neo4j driver from context (lazy-loaded)
        repo_extractor_lazy = ctx.request_context.lifespan_context.repo_extractor
        if not repo_extractor_lazy:
            return json.dumps(
                {
                    "success": False,
                    "error": "Neo4j connection not available. Check Neo4j configuration in environment variables.",
                },
                indent=2,
            )

        # Lazy-load the actual extractor
        repo_extractor = await repo_extractor_lazy.get_extractor()
        if not repo_extractor or not repo_extractor.driver:
            return json.dumps(
                {
                    "success": False,
                    "error": "Failed to initialize Neo4j extractor. Check Neo4j configuration.",
                },
                indent=2,
            )

        # Use KnowledgeGraphCommands to execute the command
        from .knowledge_graph_commands import KnowledgeGraphCommands

        cmd_handler = KnowledgeGraphCommands(repo_extractor.driver)
        return await cmd_handler.execute(command)

    except Exception as e:
        return json.dumps(
            {
                "success": False,
                "command": command,
                "error": f"Query execution failed: {str(e)}",
            },
            indent=2,
        )


@mcp.tool()
async def parse_github_repository(ctx: Context, repo_url: str) -> str:
    """
    Parse a GitHub repository into the Neo4j knowledge graph.

    This tool clones a GitHub repository, analyzes its Python files, and stores
    the code structure (classes, methods, functions, imports) in Neo4j for use
    in hallucination detection. The tool:

    - Clones the repository to a temporary location
    - Analyzes Python files to extract code structure
    - Stores classes, methods, functions, and imports in Neo4j
    - Provides detailed statistics about the parsing results
    - Automatically handles module name detection for imports

    Args:
        ctx: The MCP server provided context
        repo_url: GitHub repository URL (e.g., 'https://github.com/user/repo.git')

    Returns:
        JSON string with parsing results, statistics, and repository information
    """
    from github_utils import query_repository_statistics, build_repository_parse_response

    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps(
                {
                    "success": False,
                    "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment.",
                },
                indent=2,
            )

        # Get the repository extractor from context (lazy-loaded)
        repo_extractor_lazy = ctx.request_context.lifespan_context.repo_extractor

        if not repo_extractor_lazy:
            return json.dumps(
                {
                    "success": False,
                    "error": "Repository extractor not available. Check Neo4j configuration in environment variables.",
                },
                indent=2,
            )

        # Initialize extractor on first use
        repo_extractor = await repo_extractor_lazy.get_extractor()
        if not repo_extractor:
            return json.dumps(
                {
                    "success": False,
                    "error": "Failed to initialize repository extractor. Check Neo4j connection.",
                },
                indent=2,
            )

        # Validate repository URL
        validation = validate_github_url(repo_url)
        if not validation["valid"]:
            return json.dumps(
                {"success": False, "repo_url": repo_url, "error": validation["error"]},
                indent=2,
            )

        repo_name = validation["repo_name"]

        # Parse the repository (this includes cloning, analysis, and Neo4j storage)
        print(f"Starting repository analysis for: {repo_name}", file=sys.stderr, flush=True)
        await repo_extractor.analyze_repository(repo_url)
        print(f"Repository analysis completed for: {repo_name}", file=sys.stderr, flush=True)

        # Query Neo4j for statistics about the parsed repository
        stats = await query_repository_statistics(repo_extractor, repo_name, include_samples=True)

        if not stats:
            return json.dumps(
                {
                    "success": False,
                    "repo_url": repo_url,
                    "error": f"Repository '{repo_name}' not found in database after parsing",
                },
                indent=2,
            )

        # Build and return success response
        response = build_repository_parse_response(repo_url, repo_name, stats)
        return json.dumps(response, indent=2)

    except Exception as e:
        return json.dumps(
            {
                "success": False,
                "repo_url": repo_url,
                "error": f"Repository parsing failed: {str(e)}",
            },
            indent=2,
        )


@mcp.tool()
async def parse_github_repositories_batch(
    ctx: Context,
    repo_urls_json: str,
    max_concurrent: int = 3,
    max_retries: int = 2
) -> str:
    """
    Parse multiple GitHub repositories into Neo4j knowledge graph in parallel.

    This tool efficiently processes multiple repositories with intelligent features:
    - Parallel processing with configurable concurrency limits
    - Automatic retry logic for transient failures
    - Detailed per-repository status tracking
    - Aggregate statistics and error reporting
    - Progress visibility for long-running operations

    Perfect for:
    - Bulk importing organization repositories
    - Building comprehensive knowledge graphs
    - Recovering from partial failures
    - Large-scale code analysis projects

    Args:
        ctx: The MCP server provided context
        repo_urls_json: JSON array of GitHub repository URLs
                       Example: '["https://github.com/user/repo1.git", "https://github.com/user/repo2.git"]'
        max_concurrent: Maximum number of repositories to process simultaneously (default: 3)
                       Lower values = less memory usage, higher values = faster completion
        max_retries: Number of retry attempts for failed repositories (default: 2)
                    Set to 0 to disable retries

    Returns:
        JSON string with:
        - Overall success status
        - Per-repository results with detailed status
        - Aggregate statistics (total, successful, failed)
        - Failed repositories list for easy retry
        - Processing time metrics

    Example:
        repos = '["https://github.com/openai/openai-python.git", "https://github.com/anthropics/anthropic-sdk-python.git"]'
        parse_github_repositories_batch(repos, max_concurrent=2, max_retries=1)
    """
    import asyncio
    import time
    from github_utils import (
        validate_batch_input,
        validate_repository_urls,
        build_batch_response,
        print_batch_summary,
        process_single_repository,
    )

    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps(
                {
                    "success": False,
                    "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment.",
                },
                indent=2,
            )

        # Get the repository extractor from context (lazy-loaded)
        repo_extractor_lazy = ctx.request_context.lifespan_context.repo_extractor
        if not repo_extractor_lazy:
            return json.dumps(
                {
                    "success": False,
                    "error": "Repository extractor not available. Check Neo4j configuration.",
                },
                indent=2,
            )

        # Initialize extractor on first use
        repo_extractor = await repo_extractor_lazy.get_extractor()
        if not repo_extractor:
            return json.dumps(
                {
                    "success": False,
                    "error": "Failed to initialize repository extractor. Check Neo4j connection.",
                },
                indent=2,
            )

        # Validate and parse input parameters
        try:
            repo_urls, max_concurrent, max_retries = validate_batch_input(
                repo_urls_json, max_concurrent, max_retries
            )
        except ValueError as e:
            return json.dumps({"success": False, "error": str(e)}, indent=2)

        start_time = time.time()

        # Validate all repository URLs
        try:
            validated_repos, validation_errors = validate_repository_urls(
                repo_urls, validate_github_url
            )
        except ValueError as e:
            return json.dumps({"success": False, "error": str(e)}, indent=2)

        # Set up parallel processing
        semaphore = asyncio.Semaphore(max_concurrent)

        print(f"\nStarting batch processing of {len(validated_repos)} repositories...", file=sys.stderr, flush=True)
        print(f"Concurrency limit: {max_concurrent}, Max retries per repo: {max_retries}\n", file=sys.stderr, flush=True)

        # Process all repositories in parallel (with concurrency limit)
        tasks = [
            process_single_repository(
                repo, repo_extractor, semaphore, max_retries
            )
            for repo in validated_repos
        ]
        results = await asyncio.gather(*tasks)

        # Calculate timing and build response
        elapsed_time = time.time() - start_time
        response = build_batch_response(results, validation_errors, elapsed_time)

        # Print summary to console
        stats = response["summary"]
        print_batch_summary(
            stats["total_repositories"],
            stats["successful"],
            stats["failed"],
            stats["retried"],
        )

        return json.dumps(response, indent=2)

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"Batch processing failed: {str(e)}"
        }, indent=2)


async def crawl_markdown_file(
    crawler: AsyncWebCrawler, url: str
) -> List[Dict[str, Any]]:
    """
    Crawl a .txt or markdown file.

    Args:
        crawler: AsyncWebCrawler instance
        url: URL of the file

    Returns:
        List of dictionaries with URL and markdown content
    """
    crawl_config = CrawlerRunConfig()

    result = await crawler.arun(url=url, config=crawl_config)
    if result.success and result.markdown:
        return [{"url": url, "markdown": result.markdown}]
    else:
        print(f"Failed to crawl {url}: {result.error_message}", file=sys.stderr, flush=True)
        return []


async def crawl_batch(
    crawler: AsyncWebCrawler, urls: List[str], max_concurrent: int = 10
) -> List[Dict[str, Any]]:
    """
    Batch crawl multiple URLs in parallel.

    Args:
        crawler: AsyncWebCrawler instance
        urls: List of URLs to crawl
        max_concurrent: Maximum number of concurrent browser sessions

    Returns:
        List of dictionaries with URL and markdown content
    """
    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=max_concurrent,
    )

    results = await crawler.arun_many(
        urls=urls, config=crawl_config, dispatcher=dispatcher
    )
    return [
        {"url": r.url, "markdown": r.markdown}
        for r in results
        if r.success and r.markdown
    ]


# ============================================================================
# GraphRAG Tools - Document Knowledge Graph for Web Content
# ============================================================================


@mcp.tool()
async def crawl_with_graph_extraction(
    ctx: Context,
    url: str,
    extract_entities: bool = True,
    extract_relationships: bool = True,
    chunk_size: int = 5000
) -> str:
    """
    Crawl a URL and extract both vector embeddings (Supabase) and knowledge graph (Neo4j).

    This is the core GraphRAG crawl tool - it performs standard web crawling with
    vector embeddings PLUS extracts entities and relationships into a knowledge graph.

    Use this when you want rich, graph-augmented RAG capabilities where the system
    can understand entity relationships, dependencies, and connections.

    Args:
        ctx: The MCP server provided context
        url: URL to crawl
        extract_entities: Whether to extract entities (default: True)
        extract_relationships: Whether to extract relationships between entities (default: True)
        chunk_size: Size of text chunks for processing (default: 5000)

    Returns:
        JSON string with:
        - Crawl results (documents stored, chunks created)
        - Entity extraction results (entities found, relationships mapped)
        - Graph storage status
        - Statistics

    Example:
        crawl_with_graph_extraction("https://fastapi.tiangolo.com/")
    """
    try:
        from .graphrag_utils import (
            initialize_graphrag_components,
            generate_document_id,
            extract_source_info,
            store_graphrag_entities,
            store_graphrag_relationships,
            build_graphrag_crawl_response,
            prepare_supabase_data,
        )
        from .utils import chunk_content

        # Initialize and validate GraphRAG components
        components, error = await initialize_graphrag_components(ctx)
        if error:
            return json.dumps({"success": False, "error": error}, indent=2)

        # Extract components
        crawler = components["crawler"]
        supabase_client = components["supabase_client"]
        document_graph_validator = components["document_graph_validator"]
        document_entity_extractor = components["document_entity_extractor"]

        # Step 1: Crawl URL
        run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)
        result = await crawler.arun(url=url, config=run_config)

        if not result.success:
            return json.dumps({
                "success": False,
                "error": f"Failed to crawl URL: {result.error_message}"
            }, indent=2)

        # Step 2: Chunk content and extract source info
        chunks = chunk_content(result.markdown, max_chunk_size=chunk_size)
        source_id, title = extract_source_info(url, result.markdown)

        # Step 3: Store in Supabase (vector embeddings)
        total_word_count = len(result.markdown.split())
        source_summary = extract_source_summary(source_id, result.markdown[:5000])
        update_source_info(supabase_client, source_id, source_summary, total_word_count)

        # Prepare and store documents
        supabase_data = prepare_supabase_data(url, chunks, source_id, result.markdown)
        add_documents_to_supabase(
            client=supabase_client,
            urls=supabase_data["urls_list"],
            chunk_numbers=supabase_data["chunk_numbers"],
            contents=chunks,
            metadatas=supabase_data["metadatas"],
            url_to_full_document=supabase_data["url_to_full_document"]
        )

        # Step 4: Extract entities and relationships
        extraction_result = await document_entity_extractor.extract_entities_from_chunks(
            chunks=chunks[:10],  # Limit to first 10 chunks for performance
            max_concurrent=3
        )

        if extraction_result.error:
            return json.dumps({
                "success": False,
                "error": f"Entity extraction failed: {extraction_result.error}",
                "crawl_success": True,
                "documents_stored": len(chunks)
            }, indent=2)

        # Step 5: Store document node in Neo4j
        document_id = generate_document_id(url)
        await document_graph_validator.store_document_node(
            document_id=document_id,
            source_id=source_id,
            url=url,
            title=title
        )

        # Step 6: Store entities
        entities_stored = await store_graphrag_entities(
            document_graph_validator,
            document_id,
            extraction_result,
            extract_entities
        )

        # Step 7: Store relationships
        relationships_stored = await store_graphrag_relationships(
            document_graph_validator,
            extraction_result,
            extract_relationships
        )

        return build_graphrag_crawl_response(
            success=True,
            url=url,
            source_id=source_id,
            chunks_count=len(chunks),
            total_words=total_word_count,
            extraction_result=extraction_result,
            entities_stored=entities_stored,
            relationships_stored=relationships_stored,
            document_id=document_id
        )

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"Unexpected error: {str(e)}"
        }, indent=2)


@mcp.tool()
async def graphrag_query(
    ctx: Context,
    query: str,
    use_graph_enrichment: bool = True,
    max_entities: int = 15,
    source_filter: Optional[str] = None
) -> str:
    """
    Perform RAG query with optional graph enrichment for richer context.

    This tool combines vector similarity search (traditional RAG) with knowledge graph
    traversal (GraphRAG) to provide more comprehensive answers that understand
    entity relationships, dependencies, and connections.

    **When to use graph enrichment:**
    - Complex questions about how things relate
    - Questions about dependencies or prerequisites
    - Multi-hop reasoning questions
    - When you need to understand entity connections

    **When to disable graph enrichment (faster):**
    - Simple factual lookups
    - Time-sensitive queries
    - Very broad queries that don't need deep context

    Args:
        ctx: The MCP server provided context
        query: Search query
        use_graph_enrichment: Add graph context to results (default: True)
        max_entities: Maximum entities to include in graph enrichment (default: 15)
        source_filter: Optional source domain filter

    Returns:
        JSON string with:
        - Answer text with context
        - Source documents
        - Graph entities and relationships (if enrichment enabled)
        - Relevance scores

    Example:
        graphrag_query("How do I configure OAuth2 in FastAPI?", use_graph_enrichment=True)
    """
    try:
        supabase_client = ctx.request_context.lifespan_context.supabase_client
        document_graph_queries_lazy = ctx.request_context.lifespan_context.document_graph_queries

        # Step 1: Vector search (standard RAG)
        documents = search_documents(
            supabase_client=supabase_client,
            query=query,
            source_filter=source_filter,
            match_count=10
        )

        if not documents:
            return json.dumps({
                "success": True,
                "answer": "No relevant documents found for your query.",
                "documents": [],
                "graph_enrichment": None
            }, indent=2)

        # Step 2: Graph enrichment (if enabled and available)
        graph_context = None
        enrichment_text = ""
        document_graph_queries = None

        if use_graph_enrichment and document_graph_queries_lazy:
            # Initialize graph queries on first use
            document_graph_queries = await document_graph_queries_lazy.get_queries()

        if use_graph_enrichment and document_graph_queries:
            # Extract document IDs (would need to be stored in Supabase metadata)
            # For now, we'll use URLs as fallback
            doc_identifiers = [doc.get("url", "") for doc in documents[:5]]

            # This is a simplified version - in production you'd link Supabase IDs to Neo4j
            enrichment_text = "\n\n## Related Concepts and Dependencies\n\n"
            enrichment_text += "Graph enrichment is available. Connect Supabase document IDs to Neo4j for full GraphRAG capabilities.\n"

        # Step 3: Build context for LLM
        context_parts = []
        for i, doc in enumerate(documents[:5], 1):
            context_parts.append(f"**Source {i}:** {doc.get('url', 'Unknown')}")
            context_parts.append(doc.get('content', '')[:1000])
            context_parts.append("")

        context = "\n".join(context_parts)
        if enrichment_text:
            context = enrichment_text + "\n\n" + context

        # Step 4: Generate answer with LLM
        from openai import AsyncOpenAI
        openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that answers questions based on the provided context. If graph enrichment is included, use it to provide more comprehensive answers that explain relationships and dependencies."},
                {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}\n\nProvide a detailed answer based on the context."}
            ],
            temperature=0.3
        )

        answer = response.choices[0].message.content

        return json.dumps({
            "success": True,
            "query": query,
            "answer": answer,
            "graph_enrichment_used": use_graph_enrichment and document_graph_queries is not None,
            "documents_found": len(documents),
            "sources": [
                {
                    "url": doc.get("url", "Unknown"),
                    "relevance": doc.get("similarity", 0)
                }
                for doc in documents[:5]
            ]
        }, indent=2)

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"Query failed: {str(e)}"
        }, indent=2)


@mcp.tool()
async def query_document_graph(
    ctx: Context,
    cypher_query: str
) -> str:
    """
    Execute a custom Cypher query on the document knowledge graph.

    This tool provides direct access to the Neo4j document graph for advanced users
    who want to write custom graph queries.

    **Common query patterns:**

    Find all entities of a type:
    ```cypher
    MATCH (t:Technology)
    RETURN t.name, t.description
    LIMIT 10
    ```

    Find relationships:
    ```cypher
    MATCH (a)-[r:REQUIRES]->(b)
    RETURN a.name, type(r), b.name
    LIMIT 20
    ```

    Find entities mentioned in documents:
    ```cypher
    MATCH (d:Document)-[:MENTIONS]->(e)
    WHERE d.source_id = 'example.com'
    RETURN e.name, labels(e)[0] as type, count(*) as mentions
    ORDER BY mentions DESC
    ```

    Args:
        ctx: The MCP server provided context
        cypher_query: Cypher query string

    Returns:
        JSON string with query results

    Example:
        query_document_graph("MATCH (c:Concept) RETURN c.name LIMIT 10")
    """
    try:
        # Check if GraphRAG is enabled
        graphrag_enabled = os.getenv("USE_GRAPHRAG", "false") == "true"
        if not graphrag_enabled:
            return json.dumps({
                "success": False,
                "error": "GraphRAG functionality is disabled. Set USE_GRAPHRAG=true in environment."
            }, indent=2)

        document_graph_queries_lazy = ctx.request_context.lifespan_context.document_graph_queries

        if not document_graph_queries_lazy:
            return json.dumps({
                "success": False,
                "error": "Document graph queries not available. Check Neo4j configuration."
            }, indent=2)

        # Initialize queries on first use
        document_graph_queries = await document_graph_queries_lazy.get_queries()
        if not document_graph_queries:
            return json.dumps({
                "success": False,
                "error": "Failed to initialize document graph queries. Check Neo4j connection."
            }, indent=2)

        # Execute query
        result = await document_graph_queries.query_graph(cypher_query)

        return json.dumps(result, indent=2)

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"Query execution failed: {str(e)}"
        }, indent=2)


@mcp.tool()
async def get_entity_context(
    ctx: Context,
    entity_name: str,
    max_hops: int = 2
) -> str:
    """
    Get comprehensive context for an entity from the knowledge graph.

    This tool retrieves an entity and its neighborhood in the graph, including:
    - Entity description and type
    - Related entities (connected nodes)
    - Relationships (edges)
    - Documents that mention the entity

    Useful for:
    - Understanding what an entity is and how it relates to other concepts
    - Finding all documents that discuss a specific technology or concept
    - Exploring entity neighborhoods
    - Building context for complex questions

    Args:
        ctx: The MCP server provided context
        entity_name: Name of entity to look up (e.g., "FastAPI", "OAuth2", "Docker")
        max_hops: Maximum relationship hops to traverse (default: 2)

    Returns:
        JSON string with:
        - Entity information
        - Related entities
        - Relationships
        - Documents mentioning the entity

    Example:
        get_entity_context("FastAPI", max_hops=2)
    """
    try:
        # Check if GraphRAG is enabled
        graphrag_enabled = os.getenv("USE_GRAPHRAG", "false") == "true"
        if not graphrag_enabled:
            return json.dumps({
                "success": False,
                "error": "GraphRAG functionality is disabled. Set USE_GRAPHRAG=true in environment."
            }, indent=2)

        document_graph_queries_lazy = ctx.request_context.lifespan_context.document_graph_queries

        if not document_graph_queries_lazy:
            return json.dumps({
                "success": False,
                "error": "Document graph queries not available. Check Neo4j configuration."
            }, indent=2)

        # Initialize queries on first use
        document_graph_queries = await document_graph_queries_lazy.get_queries()
        if not document_graph_queries:
            return json.dumps({
                "success": False,
                "error": "Failed to initialize document graph queries. Check Neo4j connection."
            }, indent=2)

        # Get entity context
        context = await document_graph_queries.get_entity_context(
            entity_name=entity_name,
            max_hops=max_hops
        )

        if not context:
            return json.dumps({
                "success": False,
                "error": f"Entity '{entity_name}' not found in knowledge graph."
            }, indent=2)

        return json.dumps({
            "success": True,
            "entity": {
                "name": context.entity_name,
                "type": context.entity_type,
                "description": context.description
            },
            "related_entities": [
                {
                    "name": rel["name"],
                    "type": rel["type"],
                    "relationship": rel["relationship"]
                }
                for rel in context.related_entities
            ],
            "relationships": [
                {
                    "from": rel["from"],
                    "to": rel["to"],
                    "type": rel["type"]
                }
                for rel in context.relationships
            ],
            "documents": [
                {
                    "id": doc["id"],
                    "url": doc["url"],
                    "title": doc["title"]
                }
                for doc in context.documents
            ],
            "stats": {
                "related_entities_count": len(context.related_entities),
                "relationships_count": len(context.relationships),
                "documents_count": len(context.documents)
            }
        }, indent=2)

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"Failed to get entity context: {str(e)}"
        }, indent=2)


async def crawl_recursive_internal_links(
    crawler: AsyncWebCrawler,
    start_urls: List[str],
    max_depth: int = 3,
    max_concurrent: int = 10,
) -> List[Dict[str, Any]]:
    """
    Recursively crawl internal links from start URLs up to a maximum depth.

    Args:
        crawler: AsyncWebCrawler instance
        start_urls: List of starting URLs
        max_depth: Maximum recursion depth
        max_concurrent: Maximum number of concurrent browser sessions

    Returns:
        List of dictionaries with URL and markdown content
    """
    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=max_concurrent,
    )

    visited = set()

    def normalize_url(url):
        return urldefrag(url)[0]

    current_urls = set([normalize_url(u) for u in start_urls])
    results_all = []

    for depth in range(max_depth):
        urls_to_crawl = [
            normalize_url(url)
            for url in current_urls
            if normalize_url(url) not in visited
        ]
        if not urls_to_crawl:
            break

        results = await crawler.arun_many(
            urls=urls_to_crawl, config=run_config, dispatcher=dispatcher
        )
        next_level_urls = set()

        for result in results:
            norm_url = normalize_url(result.url)
            visited.add(norm_url)

            if result.success and result.markdown:
                results_all.append({"url": result.url, "markdown": result.markdown})
                for link in result.links.get("internal", []):
                    next_url = normalize_url(link["href"])
                    if next_url not in visited:
                        next_level_urls.add(next_url)

        current_urls = next_level_urls

    return results_all


def process_and_store_crawl_results(
    supabase_client,
    crawl_results: List[Dict[str, Any]],
    crawl_type: str,
    chunk_size: int = 5000,
) -> Dict[str, Any]:
    """
    Process crawl results and store them in Supabase.

    This helper function consolidates the common logic for processing and storing
    crawl results that is shared across multiple crawling functions.

    Args:
        supabase_client: Supabase client instance
        crawl_results: List of crawled documents with 'url' and 'markdown' keys
        crawl_type: Type of crawl (e.g., 'sitemap', 'webpage', 'text_file')
        chunk_size: Size of chunks for splitting content

    Returns:
        Dictionary with processing statistics including:
        - chunks_stored: Number of chunks stored
        - code_examples_stored: Number of code examples stored
        - sources_updated: Number of sources updated
    """
    # Import helper functions from crawl_helpers module
    from .crawl_helpers import (
        process_documentation_chunks,
        update_sources_parallel,
        extract_code_examples_from_documents,
    )

    # Step 1: Process documentation chunks
    (
        urls,
        chunk_numbers,
        contents,
        metadatas,
        url_to_full_document,
        source_content_map,
        source_word_counts,
        chunk_count
    ) = process_documentation_chunks(crawl_results, chunk_size)

    # Add crawl_type to metadata
    for meta in metadatas:
        meta["crawl_type"] = crawl_type

    # Step 2: Update source information in parallel
    update_sources_parallel(supabase_client, source_content_map, source_word_counts)

    # Step 3: Store documentation chunks in Supabase
    batch_size = 20
    add_documents_to_supabase(
        supabase_client,
        urls,
        chunk_numbers,
        contents,
        metadatas,
        url_to_full_document,
        batch_size=batch_size,
    )

    # Step 4: Extract and process code examples if enabled
    code_examples_count = 0
    extract_code_examples_enabled = os.getenv("USE_AGENTIC_RAG", "false") == "true"
    if extract_code_examples_enabled:
        (
            code_urls,
            code_chunk_numbers,
            code_examples,
            code_summaries,
            code_metadatas
        ) = extract_code_examples_from_documents(crawl_results)

        # Store code examples in Supabase
        if code_examples:
            add_code_examples_to_supabase(
                supabase_client,
                code_urls,
                code_chunk_numbers,
                code_examples,
                code_summaries,
                code_metadatas,
                batch_size=batch_size,
            )
            code_examples_count = len(code_examples)

    return {
        "chunks_stored": chunk_count,
        "code_examples_stored": code_examples_count,
        "sources_updated": len(source_content_map),
    }


async def main():
    transport = os.getenv("TRANSPORT", "sse")

    # Add health check endpoint using FastMCP's custom_route decorator
    @mcp.custom_route("/health", methods=["GET"])
    async def health_check(request):
        """Health check endpoint for monitoring and load balancers."""
        from starlette.responses import JSONResponse
        return JSONResponse({
            "status": "healthy",
            "service": "mcp-crawl4ai-rag",
            "version": "1.2.0",
            "transport": transport
        })

    if transport == "sse":
        # Run the MCP server with SSE transport
        host = os.getenv("HOST", "localhost")
        port = int(os.getenv("PORT", "8051"))
        await mcp.run_async(transport="sse", host=host, port=port)
    else:
        # Run the MCP server with stdio transport
        await mcp.run_stdio_async()


if __name__ == "__main__":
    asyncio.run(main())
